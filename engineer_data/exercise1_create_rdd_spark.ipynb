{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e6b64eb-bbb6-4b32-866d-79e5b10a2e7f",
   "metadata": {},
   "source": [
    "# Ejemplo 1: Crear un RDD con PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230222e6-15a0-4089-bd58-26bd8918116f",
   "metadata": {},
   "source": [
    "Existen dos formas de crear un **RDD**\n",
    "1. *Paralelizando* una colección ya existente\n",
    "2. Referenciando un dataset de un sistema externo como *(DataBricks File System)DBFS, HBase*, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6068fabf-81ea-49df-bf62-8f56ed580e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 2, 4]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. Creando una paralelización\n",
    "# PARALELIZACIÓN\n",
    "#SparkContext representa la conexión aun cluster de spark\n",
    "\n",
    "\n",
    "#- SparkSession.builder Es el constructor de spark \n",
    "#- appName(\"EjemploRDD\") Nombrar la app que estamos construyendo (en ls nube en el cluster)\n",
    "#- master(\"local[*]\")  Donde y cómo se va a ejecutar , para este caso en local y no en un cluster. \n",
    "#  El * indica en cual núcleo de la CPU va a correr, en este caso en todos *\n",
    "#- getOrCreate() #Verifica si existe una conexión , sino la crea\n",
    "\n",
    "#Se usa principalmente para decirle a jupyter notebook en donde se encuentra spark instalado \n",
    "#Instalamos la librería findspark `pip install findspark`\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EjemploRDD\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate() \n",
    "\n",
    "rdd = spark.sparkContext.parallelize([1,3,5,2,4,6,7,8,9])  #La colección es la lista\n",
    "\n",
    "#take(n) es como un limit en SQL o un slice [0:3] en python. Mostrando 3 elementos del rdd\n",
    "rdd.take(5)\n",
    "#top(n) Es similar a un slice [:-3] mostrando los últimos 3 registros\n",
    "#rdd.top(3)\n",
    "# takeOrdered(n) es similar a take , pero devuelve los elementos de manera ordenada, aún cuanso estén en desorden\n",
    "#rdd.takeOrdered()\n",
    "# collect() Para visualizar todos los registros del rdd\n",
    "#rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a23bb419-3cad-4c03-9df6-6d0fdcee44f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['consecutive_work_order,cant', '5677,2', '5810,2', '5740,3']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. Importanto datos desde un archivo del sistema\n",
    "#Establecemos una variable con la ruta del archivo\n",
    "file_csv = './work_order_duplicate.csv'\n",
    "\n",
    "#Pasamos el contexto de spark\n",
    "rdd2 = spark.sparkContext.textFile(file_csv)\n",
    "\n",
    "rdd2.take(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b18c98a-bd9d-4361-a160-b786bb760867",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
